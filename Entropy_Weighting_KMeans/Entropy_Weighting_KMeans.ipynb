{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from entropy_utils import u_calculation\n",
    "from entropy_utils import entropy_cost_function\n",
    "from entropy_utils import update_Z\n",
    "from entropy_utils import sub_weight_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7592608602471107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def ewkmeans(X, k, gama=1.5):\n",
    "    \"\"\"Put data (X) in k clusters based on the weight of each feature \n",
    "    which is going to be calculated based on X and a user defined parameter gama\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): input data(without label)\n",
    "        k (int): the number of cluster\n",
    "        gama (float):  user defined parameter that is used in the definition of the loss function\n",
    "\n",
    "    Return:\n",
    "        history (dict): a dictionary that contains \"U\", \"Z\", \"W\", \"cost\" for all time steps of updating. \n",
    "                        it is obvious that the values of last time step is important\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = X.shape[0] # Number of samples\n",
    "    n_features = X.shape[1] # Number of features\n",
    "    n_clusters = k # Number of clusters\n",
    "\n",
    "    # Dictionary of history\n",
    "    history = {\n",
    "        'U': [],\n",
    "        'Z': [],\n",
    "        'W': [],\n",
    "        'cost': []\n",
    "\n",
    "        }\n",
    "    # initial centers randomly by choosing from hte dataset randomly\n",
    "    Z_initial_index = np.random.choice(range(n_samples), size=n_clusters,replace=False)\n",
    "    # Centers of clusters of random samples of data, but they can be any random data_points not necessarily in dataset\n",
    "    Z = X[Z_initial_index , : ]\n",
    "    history['Z'].append(Z)\n",
    "    \n",
    "    # Generate random weights that sum up to 1\n",
    "\n",
    "    weights = (1/n_features) * np.ones((n_clusters,n_features)).squeeze()\n",
    "    # weights = np.array([1/4, 1/4, 1/4, 1/4])\n",
    "    history['W'].append(weights)\n",
    "\n",
    "    initial_U = np.zeros((X.shape[0], Z.shape[0]))\n",
    "    history['U'].append(initial_U)\n",
    "\n",
    "    # put every thing together go for a while loop\n",
    "    Z = history['Z'][-1] # the last update of Z\n",
    "    weights = history['W'][-1] # the last update of W\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # P1 --> update U\n",
    "        U = u_calculation(X, Z, weights)\n",
    "        history['U'].append(U)\n",
    "        # update cost\n",
    "        c = entropy_cost_function(U, Z, weights, X, gama)\n",
    "        history['cost'].append(c)\n",
    "        if (history['U'][-1] == history['U'][-2]).all():\n",
    "            break\n",
    "\n",
    "        #P2 --> update Z     \n",
    "        Z = update_Z(U, Z, X) # new update of Z\n",
    "        history['Z'].append(Z)\n",
    "        # Update cost\n",
    "        c = entropy_cost_function(U, Z, weights, X, gama)\n",
    "        history['cost'].append(c)\n",
    "    \n",
    "        # P3 --> update  weights\n",
    "        weights = sub_weight_update(X, U, Z, weights, gama)\n",
    "        history['W'].append(weights)\n",
    "        #update cost\n",
    "        c = entropy_cost_function(U, Z, weights, X, gama)\n",
    "        history['cost'].append(c)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    from entropy_utils import clusters_vec\n",
    "\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "\n",
    "    # Access the features (X) and target (y) data\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    n_clusters = 3\n",
    "    n_features = X.shape[1]\n",
    "    n_samples = X.shape[0]\n",
    "\n",
    "    # Normalize Data\n",
    "    def normalizer(data):\n",
    "        return (data - np.mean(data,axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n",
    "\n",
    "    X_norm = normalizer(X)\n",
    "\n",
    "\n",
    "    gama = 0.3\n",
    "    average_adjusted_rand_score_array = np.zeros(100)\n",
    "    for i in range(100):\n",
    "        history = ewkmeans(X_norm, n_clusters, gama=gama)\n",
    "        U = history[\"U\"][-1]\n",
    "        clusters = clusters_vec(U)\n",
    "        a_r_s = adjusted_rand_score(y, clusters )\n",
    "        average_adjusted_rand_score_array[i] = a_r_s\n",
    "        \n",
    "    print(average_adjusted_rand_score_array.mean())\n",
    "\n",
    "    # # First Algorithm\n",
    "\n",
    "    # average_adjusted_rand_score_array = np.zeros_like(gama_values)\n",
    "\n",
    "    # for b, gama in enumerate(gama_values):\n",
    "    #     adjusted_rand_score_array = np.zeros(repetition)\n",
    "    #     for i in range(repetition):\n",
    "    #         history = ewkmeans(X_norm, n_clusters, gama)\n",
    "    #         U = history[\"U\"][-1]\n",
    "    #         clusters = clusters_vec(U)\n",
    "    #         a_r_s = adjusted_rand_score(y, clusters )\n",
    "    #         adjusted_rand_score_array[i] = a_r_s\n",
    "\n",
    "    #     average_adjusted_rand_score_array[b] =  adjusted_rand_score_array.mean()\n",
    "    #     # print(average_adjusted_rand_score_array)\n",
    "    #     # print(history[\"cost\"])\n",
    "\n",
    "    # # Plot the data\n",
    "    # plt.plot(gama_values,average_adjusted_rand_score_array )\n",
    "\n",
    "    # # Add labels and title\n",
    "    # plt.xlabel('gama values')\n",
    "    # plt.ylabel(f'Average of Adjusted Rand Score for {repetition} repetition of gama')\n",
    "    # plt.title('First Algorithm ')\n",
    "\n",
    "    # # Show the plot\n",
    "    # plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
