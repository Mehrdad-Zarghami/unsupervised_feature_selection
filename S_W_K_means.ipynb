{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from utils_sub import u_calculation\n",
    "from utils import cost_function\n",
    "from utils import update_Z\n",
    "from utils import sub_weight_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def weighted_distance(s1, s2, weight_vec, beta):\n",
    "    \"\"\"Calculate the weighted distance between two samples s1 and  s2 and based on the weights that each feature has\n",
    "\n",
    "    Args:\n",
    "        s1 (ndarray): _description_\n",
    "        s2 (ndarray): _description_\n",
    "        weight_vec (ndarray): a vector of size (n_features, ), each element is the weight of corresponding feature.\n",
    "        beta (scaler): the power of weights vector\n",
    "\n",
    "    Returns:\n",
    "        scaler: the weighted distance between two samples\n",
    "    \"\"\"\n",
    "    distance_vec = np.square(s1 - s2) # Element_wise --> for each feature\n",
    "    # w**beta\n",
    "    weights_beta_vector = np.power(weight_vec, beta)\n",
    "\n",
    "    weighted_distance = np.dot(distance_vec, weights_beta_vector.T)\n",
    "    return weighted_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_closest_center(sample, centers, weight_matrix, beta=2):\n",
    "    \"\"\"it takes a sample and compare its distance to centers of clusters and return the cluster with closest center.\n",
    "\n",
    "    Args:\n",
    "        sample (ndarray): A vector that represent a data point\n",
    "        centers (ndarray): A ndarray with the shape of (n_clusters, n_features) where each row represent a center of a cluster\n",
    "        weight_vector (ndarray): a vector of wights for corresponding feature\n",
    "        beta (scaler): a bridge that bring\n",
    "    Returns:\n",
    "        int: the number of cluster which is closest to the samples\n",
    "    \"\"\"\n",
    "    d = [] # list of weighted distances\n",
    "    for ind, c in enumerate(centers):\n",
    "        w_d = weighted_distance(sample, c, weight_matrix[ind], beta)# --> for each cluster, its specific set of weights are used \n",
    "        d.append(w_d)\n",
    "    assigned_cluster = np.argmin(d)\n",
    "    return assigned_cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def u_calculation(data, centers, weights, beta = 2):\n",
    "   \"\"\" Calculate U based on Z and W and our dataset\n",
    "   \"\"\"\n",
    "   n_spl = data.shape[0] # umber of samples\n",
    "   n_clu = centers.shape[0] # number of clusters\n",
    "   u_matrix = np.zeros((n_spl, n_clu))\n",
    "   for i, x in enumerate(data):\n",
    "      l = sub_closest_center(x, centers, weights, beta)\n",
    "      u_matrix[i,l] = 1\n",
    "   return u_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_vec(U):\n",
    "    # a vector of size (n_samples, ) which each element shows the cluster that each samples is assigned to\n",
    "    n_samples = U.shape[0]\n",
    "    c_vec = np.zeros(n_samples)\n",
    "    for m, u in enumerate(U):\n",
    "        c_vec[m] = np.argmax(u)\n",
    "    c_vec = c_vec.astype(int)\n",
    "    return c_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_vec(U):\n",
    "    # a vector of size (n_samples, ) which each element shows the cluster that each samples is assigned to\n",
    "    n_samples = U.shape[0]\n",
    "    c_vec = np.zeros(n_samples)\n",
    "    for m, u in enumerate(U):\n",
    "        c_vec[m] = np.argmax(u)\n",
    "    c_vec = c_vec.astype(int)\n",
    "    return c_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(U, Z, W_matrix, X, beta = 2):\n",
    "    \"\"\"Calculate the cost function\n",
    "\n",
    "    Args:\n",
    "        U (ndarray):  U is an (M, k) matrix, ui,l is a binary variable, and ui,l = 1 indicates that record i is allocated to cluster l.\n",
    "        Z (ndarray): is a set of k vectors representing the k-cluster centers of size (n_clusters, n_features)\n",
    "        W_matrix (ndarray): a matrix of size (n_clusters, n_features) so that each row is W = [w1, w2, ..., wN ] is a set of weights  for cluster c.\n",
    "        X (ndarray): matrix of records (n_records, n_features)\n",
    "        beta (int, optional): The power of elements of weights vector Defaults to 2.\n",
    "    \"\"\"\n",
    "    P = 0 # initial value of cost\n",
    "\n",
    "    cl_vec = clusters_vec(U)\n",
    "    \n",
    "    # Updating P\n",
    "    for m, c in enumerate(cl_vec):\n",
    "        w_d = weighted_distance(X[m], Z[c], W_matrix[c], beta)\n",
    "        P += w_d\n",
    "        P =  P.item() # to convert it to a single scaler\n",
    "    return(P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clusters_dict(U):\n",
    "    # Finding the index of samples in each cluster\n",
    "    n_clusters = U.shape[1]\n",
    "    cluster_dict = {}\n",
    "    clu_vec = clusters_vec(U)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        cluster_dict[i] = np.where(clu_vec == i)[0]\n",
    "        \n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_Z(U, Z, X):\n",
    "    \"\"\"Update Z i.e. the centers of clusters, by taking mean of teh samples in each cluster\n",
    "    \"\"\"\n",
    "    cluster_dict = clusters_dict(U)\n",
    "\n",
    "    new_Z = np.zeros_like(Z)\n",
    "    for i,ind in cluster_dict.items():\n",
    "        new_Z[i] = np.mean(X[ind], axis=0)\n",
    "    \n",
    "    return new_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def dj(X, U, Z):\n",
    "    # Iteration over all features to calculate Dj for each feature\n",
    "    \n",
    "    cluster_dict = clusters_dict(U)\n",
    "    n_features = X.shape[1]\n",
    "    n_clusters = U.shape[1]\n",
    "    \n",
    "    D = []\n",
    "    for j in range(n_features):\n",
    "        d_j = 0\n",
    "        for l in range(n_clusters):\n",
    "            inx_in_cluster = cluster_dict[l]\n",
    "            # Distance for feature \"j\" in cluster \"l\"\n",
    "            d_j_l =np.sum(np.square(X[inx_in_cluster][j]-Z[l][j]))\n",
    "            d_j += d_j_l\n",
    "\n",
    "        D.append(d_j)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_update(X, U, Z, weights, beta=2):\n",
    "\n",
    "    # D calculation:\n",
    "    D = dj(X, U, Z)\n",
    "\n",
    "    \n",
    "    # weights_update\n",
    "    weights_upd = np.zeros_like(weights)\n",
    "\n",
    "\n",
    "    # wherever D is zero, the corresponding weight is zero\n",
    "    ind_D_zero = np.where(D == 0 )[0] # indexes of zero Dj\n",
    "    weights_upd[ind_D_zero] = 0\n",
    "\n",
    "    # D is not zero\n",
    "    ind_D_not_zero = np.where(D)[0] ## indexes of non-zero Dj\n",
    "    for j in ind_D_not_zero:\n",
    "        \n",
    "        Dj_Dt = 0\n",
    "        for t in ind_D_not_zero:\n",
    "            Dj_Dt += (D[j] / D[t]) ** (1 / ( beta - 1) )\n",
    "        \n",
    "        weights_upd[j] = 1 / Dj_Dt\n",
    "\n",
    "    return weights_upd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#################################\n",
    "# for subspace weighted k_means:\n",
    "\n",
    "\n",
    "def sub_dj(X, U, Z):\n",
    "    \"\"\" Iteration over all features to calculate D (dispersion) for each feature in each subspace or cluster\n",
    "\n",
    "    Args:\n",
    "        U (ndarray):  U is an (M, k) matrix, ui,l is a binary variable, and ui,l = 1 indicates that record i is allocated to cluster l.\n",
    "        Z (ndarray): is a set of k vectors representing the k-cluster centers of size (n_clusters, n_features)\n",
    "        X (ndarray): matrix of records (n_records, n_features)\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        D(nd.array): a  matrix of size (n_clusters, n_features), where element [l,j] is the dispersion for cluster l and feature j.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    cluster_dict = clusters_dict(U)\n",
    "    n_features = X.shape[1]\n",
    "    n_clusters = U.shape[1]\n",
    "    \n",
    "    D = np.empty((n_clusters,n_features)) # each row is for each cluster and each column is for each feature\n",
    "    for l in range(n_clusters):\n",
    "        inx_in_cluster = cluster_dict[l]\n",
    "        for j in range(n_features):\n",
    "            # Distance for feature \"j\" in cluster \"l\"\n",
    "            D[l, j] =np.sum(np.square(X[inx_in_cluster][j]-Z[l][j]))\n",
    "\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_weight_update(X, U, Z, weights, beta=2):\n",
    "    \n",
    "    n_clusters = weights.shape[0]\n",
    "    n_features = weights.shape[1]\n",
    "\n",
    "\n",
    "    # D calculation:\n",
    "    D = sub_dj(X, U, Z)\n",
    "\n",
    "    # weights_update\n",
    "    weights_upd = np.empty_like(weights) # a matrix of size (n_clusters, n_features)\n",
    "\n",
    "    for l in range(n_clusters):\n",
    "        for j in range(n_features):\n",
    "            # calculation of sum of {( D[lj] / D[lt] ) ** (1 / ( beta - 1) )} for all t, 1 < t < n_features\n",
    "            Dlj_Dlt = 0\n",
    "            for t in range(n_features):\n",
    "                Dlj_Dlt += (D[l,j] / D[l,t]) ** (1 / ( beta - 1) )\n",
    "            \n",
    "            weights_upd[l, j] = 1 / Dlj_Dlt\n",
    "\n",
    "    return weights_upd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_w_k_means(X, k, beta=2):\n",
    "    \"\"\"Put data (X) in k clusters based on the weight of each feature \n",
    "    which is going to be calculated based on X and a user defined parameter beta\n",
    "\n",
    "    Args:\n",
    "        X (ndarray): input data(without label)\n",
    "        k (int): the number of cluster\n",
    "        beta (float):  user defined parameter that is used in the definition of the loss function\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = X.shape[0] # Number of samples\n",
    "    n_features = X.shape[1] # Number of features\n",
    "    n_clusters = k # Number of clusters\n",
    "\n",
    "    # Dictionary of history\n",
    "    history = {\n",
    "        'U': [],\n",
    "        'Z': [],\n",
    "        'W': [],\n",
    "        'cost': []\n",
    "        }\n",
    "    time_step = 0\n",
    "\n",
    "    # initial centers randomly by choosing from hte dataset randomly\n",
    "    Z_initial_index = np.random.choice(range(n_samples), size=n_clusters)\n",
    "    # Centers of clusters of random samples of data, but they can be any random data_points not necessarily in dataset\n",
    "    Z = X[Z_initial_index , : ]\n",
    "    history['Z'].append(Z)\n",
    "\n",
    "    # Generate random weights that sum up to 1\n",
    "    weights = np.random.dirichlet(np.ones(n_features), size=n_clusters)\n",
    "    # weights = np.random.dirichlet(np.ones(n_features), size=1).squeeze()\n",
    "    history['W'].append(weights)\n",
    "\n",
    "    #Calculating  U\n",
    "    U = u_calculation(X, Z, weights)\n",
    "    history['U'].append(U)\n",
    "\n",
    "    #Calculating cost function\n",
    "    c_t = cost_function(U, Z, weights, X, beta)\n",
    "    history['cost'].append(c_t)\n",
    "\n",
    "    Z_t = update_Z(U, Z, X) # new update of Z\n",
    "    history['Z'].append(Z_t)\n",
    "\n",
    "    # Update cost\n",
    "    c_t = cost_function(U, Z_t, weights, X, beta) \n",
    "    history['cost'].append(c_t)\n",
    "\n",
    "    # weight update\n",
    "    weights_t = sub_weight_update(X, U, Z, weights, beta=2)\n",
    "    history['W'].append(weights_t)\n",
    "\n",
    "    c_t = cost_function(U, Z, weights_t, X, beta = 2)\n",
    "    history['cost'].append(c_t)\n",
    "\n",
    "\n",
    "\n",
    "    # put every thing together go for a while loop\n",
    "    cost_difference = []\n",
    "\n",
    "    while True:\n",
    "        cost_difference = np.abs(history['cost'][-1] - history['cost'][-2])\n",
    "        if  cost_difference > 0.0001:\n",
    "\n",
    "            # P1 --> update U\n",
    "            Z = history['Z'][-1] # the last update of Z\n",
    "            weights = history['W'][-1] # the last update of W\n",
    "            U = u_calculation(X, Z, weights)\n",
    "            history['U'].append(U)\n",
    "            U = history['U'][-1]\n",
    "            # update cost\n",
    "            c_t = cost_function(U, Z, weights, X, beta)\n",
    "            history['cost'].append(c_t)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "        #P2 --> update Z\n",
    "        cost_difference = np.abs(history['cost'][-1] - history['cost'][-2])\n",
    "        if  cost_difference > 0.0001:\n",
    "            U = history['U'][-1] # the last update of U\n",
    "            Z = history['Z'][-1] # the last update of Z\n",
    "            weights = history['W'][-1] # the last update of weights\n",
    "\n",
    "            Z_t = update_Z(U, Z, X) # new update of Z\n",
    "            history['Z'].append(Z_t)\n",
    "            # Update cost\n",
    "            c_t = cost_function(U, Z_t, weights, X, beta) \n",
    "            history['cost'].append(c_t)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "        # P3 --> update  weights\n",
    "        cost_difference = np.abs(history['cost'][-1] - history['cost'][-2])\n",
    "        if  cost_difference > 0.0001:\n",
    "            U = history['U'][-1] # the lsat update of U\n",
    "            Z = history['Z'][-1] # the lsat update of Z\n",
    "            weights_t = sub_weight_update(X, U, Z, weights, beta)\n",
    "            history['W'].append(weights_t)\n",
    "            #update cost\n",
    "            c_t = cost_function(U, Z, weights_t, X, beta)\n",
    "            history['cost'].append(c_t)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    \n",
    "    return history\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     # producing clustering dataset\n",
    "#     from sklearn.datasets import make_blobs\n",
    "\n",
    "#     n_samples = 200\n",
    "#     n_features = 6\n",
    "#     n_clusters = 3\n",
    "#     data, y = make_blobs(n_samples=n_samples, n_features=n_features , centers=n_clusters, random_state=42)\n",
    "\n",
    "#     hist = sub_w_k_means(data, n_clusters, beta=2)\n",
    "#     print(hist['W'][-1])\n",
    "#     print(np.sum(hist['W'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25999461 0.09041776 0.18065545 0.23687374 0.12249849 0.10955995]\n",
      " [0.09315055 0.17037752 0.30061339 0.21207378 0.09579951 0.12798525]\n",
      " [0.05832977 0.18638139 0.15280761 0.16155574 0.27562388 0.16530161]]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "hist = sub_w_k_means(data, n_clusters, beta=2)\n",
    "print(hist['W'][-1])\n",
    "print(np.sum(hist['W'][-1], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsupervised",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
